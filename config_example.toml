# $schema: ./schemas/config.schema.json
# This file is compatible with the parisneo toml hilighter extension for vscode:
# https://marketplace.visualstudio.com/items?itemName=parisneo.toml-hilighter&ssr=false#overview
# This uses a schema to enforce the inputs
# config.toml
[server]
host = "0.0.0.0"
port = "9642"

[app_settings]
data_dir = "data"  # For user-specific discussion files and SafeStore DBs
database_url = "sqlite:///./data/app_main.db" # Central SQLite DB for users and app settings
secret_key = "a_very_secret_key_for_flask_sessions_or_jwt" # For session/token use
access_token_expires_mintes = 44640 # How many minutes the access token stays alive after last use (44640 = 1 month)

# Initial admin user to be created in the database if it doesn't exist.
# The password here should be the PLAIN TEXT password you want for the first admin.
# It will be hashed and stored in the database on first startup.
# For subsequent logins, the database's hashed password will be used.
[initial_admin_user]
username = "superadmin"
password = "CHANGE_ME_TO_A_STRONG_PASSWORD" # This will be hashed on first setup

[lollms_client_defaults] # Global defaults, can be overridden by user settings in DB
# ollama
# binding_name = "ollama" # Example: "ollama", "openai", "lollms", "transformers"
# default_model_name = "phi4:latest" # Example: "phi3:latest" for ollama, "gpt-4o-mini" for openai
# host_address = "http://localhost:11434" # e.g., "http://localhost:11434" for Ollama, "https://api.openai.com/v1" for OpenAI
# service_key_env_var = "OPENAI_API_KEY" # Environment variable name for API key (e.g., for OpenAI)
# ctx_size = 32000 # Default context size for LLM
# user_name = "user" # Default name for the user in prompts
# ai_name = "assistant" # Default name for the AI in prompts

# llamacpp
binding_name = "llamacpp" # Example: "ollama", "openai", "lollms", "transformers"
default_model_name = "llava-v1.6-mistral-7b.Q3_K_XS.gguf" # Example: "phi3:latest" for ollama, "gpt-4o-mini" for openai
models_path = "E:\\drumber" # e.g., "http://localhost:11434" for Ollama, "https://api.openai.com/v1" for OpenAI
host_address = "http://localhost:11434" # e.g., "http://localhost:11434" for Ollama, "https://api.openai.com/v1" for OpenAI
service_key_env_var = "OPENAI_API_KEY" # Environment variable name for API key (e.g., for OpenAI)
ctx_size = 32000 # Default context size for LLM
user_name = "user" # Default name for the user in prompts
ai_name = "assistant" # Default name for the AI in prompts


# temperature = 0.7 # Optional: Default temperature
# top_k = 50 # Optional: Default top_k
# top_p = 0.95 # Optional: Default top_p

[safe_store_defaults] # Global defaults
chunk_size = 2096 # characters (as safestore make no assumptions about the LLM)
chunk_overlap = 150 # characters (as safestore make no assumptions about the LLM)
# Default vectorizer if not set per user. Users can override this.
global_default_vectorizer = "st:all-MiniLM-L6-v2" # Example Sentence Transformer model
# Global encryption key for user SafeStores.
# For per-user encryption keys, you'd store them encrypted in the UserDB (not implemented here).
# If set, requires 'safe_store[encryption]' -> 'cryptography' to be installed.
encryption_key = "my_super_secret_safe_store_key_CHANGE_ME" # Or null/remove to disable encryption
