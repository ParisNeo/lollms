# $schema: ./schemas/config.schema.json
# This file is compatible with the parisneo toml hilighter extension for vscode:
# https://marketplace.visualstudio.com/items?itemName=parisneo.toml-hilighter&ssr=false#overview
# This uses a schema to enforce the inputs
# config.toml
[server]
host = "0.0.0.0"
port = "9642"
debug = false
migrate = false # micration from old version using yaml
https_enabled = false
ssl_certfile = ""
ssl_keyfile = ""

[app_settings]
data_dir = "data"  # For user-specific discussion files and SafeStore DBs
database_url = "sqlite:///./data/app_main.db" # Central SQLite DB for users and app settings
secret_key = "a_very_secret_key_for_flask_sessions_or_jwt" # For session/token use
access_token_expires_mintes = 44640 # How many minutes the access token stays alive after last use (44640 = 1 month)

# Initial admin user to be created in the database if it doesn't exist.
# The password here should be the PLAIN TEXT password you want for the first admin.
# It will be hashed and stored in the database on first startup.
# For subsequent logins, the database's hashed password will be used.
[initial_admin_user]
username = "superadmin"
password = "CHANGE_ME_TO_A_STRONG_PASSWORD" # This will be hashed on first setup

# --- IMPORTANT ---
# This section is now used ONLY for the very first startup to create an initial
# LLM binding in the database. After the first run, all binding configurations
# are managed exclusively through the Admin Panel in the web UI.
# Any changes made to this section after the first run will have NO EFFECT.
[lollms_client_defaults] # Global defaults, can be overridden by user settings in DB
# ollama
# binding_name = "ollama" # Example: "ollama", "openai", "lollms", "transformers", "litellm", "gemini"
# default_model_name = "phi4:latest" # Example: "phi3:latest" for ollama, "gpt-4o-mini" for openai
# host_address = "http://localhost:11434" # e.g., "http://localhost:11434" for Ollama, "https://api.openai.com/v1" for OpenAI
# service_key = "OPENAI_API_KEY" # Environment variable name for API key (e.g., for OpenAI)
# ctx_size = 32000 # Default context size for LLM
# user_name = "user" # Default name for the user in prompts
# ai_name = "assistant" # Default name for the AI in prompts

# llamacpp
binding_name = "llamacpp" # Example: "ollama", "openai", "lollms", "transformers", "litellm", "gemini"
default_model_name = "llava-v1.6-mistral-7b.Q3_K_XS.gguf" # Example: "phi3:latest" for ollama, "gpt-4o-mini" for openai
models_path = "E:\\drumber" # e.g., "http://localhost:11434" for Ollama, "https://api.openai.com/v1" for OpenAI
host_address = "http://localhost:11434" # e.g., "http://localhost:11434" for Ollama, "https://api.openai.com/v1" for OpenAI
service_key = "OLLAMA_API_KEY" # API key (if you use a proxy like ollama_proxy_server)
ctx_size = 32000 # Default context size for LLM
user_name = "user" # Default name for the user in prompts
ai_name = "assistant" # Default name for the AI in prompts

# temperature = 0.7 # Optional: Default temperature
# top_k = 50 # Optional: Default top_k
# top_p = 0.95 # Optional: Default top_p

[safe_store_defaults] # Global defaults
chunk_size = 2096 # characters (as safestore make no assumptions about the LLM)
chunk_overlap = 150 # characters (as safestore make no assumptions about the LLM)
# Default vectorizer if not set per user. Users can override this.
global_default_vectorizer = "st:all-MiniLM-L6-v2" # Example Sentence Transformer model
# Global encryption key for user SafeStores.
# For per-user encryption keys, you'd store them encrypted in the UserDB (not implemented here).
# If set, requires 'safe_store[encryption]' -> 'cryptography' to be installed.
encryption_key = "my_super_secret_safe_store_key_CHANGE_ME" # Or null/remove to disable encryption

#[[default_mcps]]
#name = "lollms_official_tools" # an alias fot he server
#url = "http://lollms.org:9602" # the address of the server

[[default_personas]]
name = "Standard Assistant"
category = "General"
author = "System"
description = "A helpful and general-purpose AI assistant."
prompt_text = "You are a helpful AI assistant. Respond clearly and concisely."
is_public = true
icon_base64 = "<Here you can put a base64 icon>"

[[default_personas]]
name = "Think First"
category = "General"
author = "ParisNeo"
description = "An assistant that pauses to think inside <think></think> before answering."
prompt_text = "You are an AI that first reasons in <think></think> tags, then responds to the user clearly and logically."
is_public = true
icon_base64 = "<Here you can put a base64 icon>"